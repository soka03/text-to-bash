{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2ffb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers bitsandbytes datasets sentencepiece accelerate peft flash-attn wandb openai pqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a4e378-3792-45c2-aef7-3b105610475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (4.13.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: trl==0.9.6 in /usr/local/lib/python3.11/dist-packages (0.9.6)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (2.4.1+cu124)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (4.51.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (1.26.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (1.6.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (3.6.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (0.9.19)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.31.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (4.67.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (14.0.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->trl==0.9.6) (6.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (3.11.18)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.31.0->trl==0.9.6) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U typing_extensions\n",
    "!pip install trl==0.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd1fcb8-c3e5-41a2-93bd-895b19bbecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc022a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import trl\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "\n",
    "import wandb\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          pipeline)\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18bc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version       : 2.4.1+cu124\n",
      "Transformers version  : 4.51.3\n",
      "TRL version           : 0.9.6\n",
      "CUDA available        : True\n",
      "CUDA version      : 12.4\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version       : {torch.__version__}\")\n",
    "print(f\"Transformers version  : {transformers.__version__}\")\n",
    "print(f\"TRL version           : {trl.__version__}\")\n",
    "print(f\"CUDA available        : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version      : {torch.version.cuda}\")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "login(\n",
    "  token=HUGGINGFACE_TOKEN,\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e52a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dce3b468e3b4dfe8108768e8708320b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ef283291dc42df981a156dd9fc07f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/939k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c5d42106cb4b8f8b19b51c06c16362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 16178\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"rlawltjd/korean_nl2bash_augmented-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33fb13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3b23d61b344d0ea7d302802b806674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ba7d0863564d4abe87eefe9df44d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e12a275a4848178245f907e55bb24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807d581638944d7b9019b5862eef3e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13751 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828378b34ee3403c91b58ba165b0dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469fb5e6b21c40969934cb44224f58da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/809 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c24089948b4cc582a64bb97e4f181d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be0e1280c7844febe1703a5c4a8d20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dc4466a3874978bdd13bfdd2181078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful programmer assistant that excels at changing Korean text to Bash.', 'role': 'system'}, {'content': 'Task: 모든 파일과 하위 디렉토리의 권한, 시간, 소유권을 유지하면서 현재 디렉토리의 전체 내용을 다른 디렉토리로 복사합니다.', 'role': 'user'}, {'content': 'find . | cpio -pdumv /path/to/destination/dir<|end_of_text|>', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allganize/Llama-3-Alpha-Ko-8B-Instruct\")\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "def get_chat_format(element):\n",
    "    system_prompt = \"You are a helpful programmer assistant that excels at changing Korean text to Bash.\"\n",
    "    user_prompt = \"Task: {instruction}\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format_map(element)},\n",
    "            {\"role\": \"assistant\", \"content\": element[\"output\"] + tokenizer.eos_token},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "base = dataset[\"train\"]  \n",
    "\n",
    "split_1 = base.train_test_split(test_size=0.15, seed=42)\n",
    "train_set = split_1[\"train\"]\n",
    "temp_set = split_1[\"test\"]\n",
    "\n",
    "split_2 = temp_set.train_test_split(test_size=1/3, seed=42)\n",
    "valid_set = split_2[\"train\"]\n",
    "test_set = split_2[\"test\"]\n",
    "\n",
    "train_set = train_set.map(get_chat_format, remove_columns=[\"instruction\", \"output\"])\n",
    "valid_set = valid_set.map(get_chat_format, remove_columns=[\"instruction\", \"output\"])\n",
    "test_set  = test_set.map(get_chat_format, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_set,\n",
    "    \"validation\": valid_set,\n",
    "    \"test\": test_set\n",
    "})\n",
    "\n",
    "# 5. 저장\n",
    "final_dataset[\"train\"].to_json(\"./dataset-16000/train_dataset.json\", orient=\"records\")\n",
    "final_dataset[\"validation\"].to_json(\"./dataset-16000/valid_dataset.json\", orient=\"records\")\n",
    "final_dataset[\"test\"].to_json(\"./dataset-16000/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(final_dataset[\"train\"][345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8012c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 13751\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 1618\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 809\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[\"train\"], final_dataset['validation'], final_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36515f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful programmer assistant that excels at changing Korean text to Bash.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Task: 현재 디렉토리 아래의 모든 디렉토리를 찾아 소유자에게 읽기, 쓰기, 실행 권한을 부여하고, 그룹에게는 읽기 및 실행 권한을 부여하며, 다른 사용자에게는 실행 권한만 부여하도록 설정해라.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'find . -type d -exec chmod u=rwx,g=rx,o=x {} \\\\;<|end_of_text|>',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475cdbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439ac6d8d91e4236a4d561b4a8d6b229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc16b49260cd4101b18df4c2f24462c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa0d380be0849969a7d0a27077e7bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"./dataset-16000/train_dataset.json\",\n",
    "        \"validation\": \"./dataset-16000/valid_dataset.json\",\n",
    "        \"test\": \"./dataset-16000/test_dataset.json\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb30fb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f13d52daf4749d6a31b53bd4fb28a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantization config 세팅 -> 모델이 사용하는 vram을 최소화\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "model_id = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspace/llama3-ko\",                                        \n",
    "    device_map=\"auto\",                            \n",
    "    attn_implementation=\"flash_attention_2\",        \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/workspace/llama3-ko\", local_files_only=True)\n",
    "tokenizer.padding_side = 'right'                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "524576d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bfb248ddca442489f0aed9ac24b25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df262326d02c461783c380f416e32632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,                            \n",
    "        lora_dropout=0.05,                         \n",
    "        r=256,                                     # Lora의 저차원 공간의 랭크를 지정. 랭크가 높을수록 모델의 표현력과과 계산 비용도 증가.\n",
    "        bias=\"none\",                               # Lora 적용 시 바이어스를 사용할지 여부를 설정. \n",
    "        target_modules=[\"q_proj\", \"o_proj\",        # Lora를 적용할 모델의 모듈 리스트\n",
    "                        \"k_proj\", \"v_proj\",\n",
    "                        \"up_proj\", \"down_proj\",\n",
    "                        \"gate_proj\",\n",
    "                        ],\n",
    "        task_type=\"CAUSAL_LM\",                    \n",
    ")\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"code-llama3-7b-text-to-bash-data-augmented-v3\", \n",
    "    num_train_epochs=3,                   \n",
    "    # max_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    per_device_train_batch_size=1,         \n",
    "    gradient_accumulation_steps=2,          \n",
    "    gradient_checkpointing=True,            \n",
    "    optim=\"adamw_torch_fused\",              # 메모리 효율화할 수 있는 fused AdamW 옵티마이저 사용.\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=1e-4,                     \n",
    "    bf16=True,                              \n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,                      \n",
    "    warmup_ratio=0.03,                      # 워밍업 비율 0.03으로 설정 (QLoRA 논문 기반).\n",
    "    lr_scheduler_type=\"cosine\",           \n",
    "    push_to_hub=True,                       \n",
    "    report_to=\"wandb\",                      \n",
    ")\n",
    "\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,                     \n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  \n",
    "        \"append_concat_token\": False, \n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd4a8f2-431e-40c6-9fb7-4c4bc3d6cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoka27\u001b[0m (\u001b[33msoka27-hufs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/kotext-to-bash/wandb/run-20250508_112956-65frw32r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/soka27-hufs/huggingface/runs/65frw32r' target=\"_blank\">code-llama3-7b-text-to-bash-data-augmented-v3</a></strong> to <a href='https://wandb.ai/soka27-hufs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/soka27-hufs/huggingface' target=\"_blank\">https://wandb.ai/soka27-hufs/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/soka27-hufs/huggingface/runs/65frw32r' target=\"_blank\">https://wandb.ai/soka27-hufs/huggingface/runs/65frw32r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='366' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [366/366 21:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>0.645013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.557904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 500 thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/18/a9/18a94e1bfbd82d64a70cacce65555cbcac9fa29a02510568c2e9443fe5d99877/dcf6056f16f04012f12e787177fbbc683ef7dcc67ba056fd4182d549abf744b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250508%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250508T113705Z&X-Amz-Expires=86400&X-Amz-Signature=52e6bf4f6bcc46b4ac696bddd3bdd2d80c5ab90ac9d258894964e1f165424210&X-Amz-SignedHeaders=host&partNumber=55&uploadId=h9ubVBS9gtDOYxcMR2eXzNSuaWttgBW73FZSdQuHgFoyBsPPxL0fQKeMBWVHrtOJM4tTFS_.sPrCRRKCIRM6kHGjiJJcvQDgKgvE95kBtbM5_vCLNM2vNC8zFl3kaoaW&x-id=UploadPart\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 500 thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/18/a9/18a94e1bfbd82d64a70cacce65555cbcac9fa29a02510568c2e9443fe5d99877/dcf6056f16f04012f12e787177fbbc683ef7dcc67ba056fd4182d549abf744b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250508%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250508T113705Z&X-Amz-Expires=86400&X-Amz-Signature=52e6bf4f6bcc46b4ac696bddd3bdd2d80c5ab90ac9d258894964e1f165424210&X-Amz-SignedHeaders=host&partNumber=55&uploadId=h9ubVBS9gtDOYxcMR2eXzNSuaWttgBW73FZSdQuHgFoyBsPPxL0fQKeMBWVHrtOJM4tTFS_.sPrCRRKCIRM6kHGjiJJcvQDgKgvE95kBtbM5_vCLNM2vNC8zFl3kaoaW&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=366, training_loss=0.5504271593250212, metrics={'train_runtime': 1276.1813, 'train_samples_per_second': 0.576, 'train_steps_per_second': 0.287, 'total_flos': 1.4668160330366976e+17, 'train_loss': 0.5504271593250212, 'epoch': 2.979591836734694})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b455799e-be39-4586-bdc2-cfd42b5ad4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b26cfda98245c2be2ff951f4d817a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec69dcebe85c4a6190e36a593fc988c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6faffc88bd426699f5a5da22fc6ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477ca71a98b04e018b99f3acaa8ca751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rlawltjd/code-llama3-7b-text-to-bash-data-augmented-v3/commit/5e0c28a20790ab0bf9da6b13fc359a4ea8cab801', commit_message='Upload tokenizer', commit_description='', oid='5e0c28a20790ab0bf9da6b13fc359a4ea8cab801', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rlawltjd/code-llama3-7b-text-to-bash-data-augmented-v3', endpoint='https://huggingface.co', repo_type='model', repo_id='rlawltjd/code-llama3-7b-text-to-bash-data-augmented-v3'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"rlawltjd/code-llama3-7b-text-to-bash-data-augmented-v3\")\n",
    "tokenizer.push_to_hub(\"rlawltjd/code-llama3-7b-text-to-bash-data-augmented-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd89951-b36d-42d9-a2bb-b3ae8faab5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d31efa-b17b-423f-ab83-973da3be0200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811e6b5af8ad4fe686d59c3b423c447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"./code-llama3-7b-text-to-bash-data-augmented-v3\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5570c1a6-5e6c-4193-8861-2d8e845050b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Task: 권한이 111보다 작은 일반 파일을 찾습니다.\n",
      "Original Answer:\n",
      "find -perm -111 -type f\n",
      "Generated Answer:\n",
      "find. -type f! -perm 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = load_dataset(\"json\", data_files=\"./dataset-16000/test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    eval_dataset[rand_idx][\"messages\"][:2], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "outputs = pipe(prompt, \n",
    "               max_new_tokens=256, \n",
    "               do_sample=False, \n",
    "               temperature=0.1, \n",
    "               top_k=50, \n",
    "               top_p=0.1, \n",
    "               eos_token_id=pipe.tokenizer.eos_token_id, \n",
    "               pad_token_id=pipe.tokenizer.pad_token_id\n",
    "               )\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\".replace(\"<|end_of_text|>\", \"\"))\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "eval_dataset[rand_idx]['messages'][2]['content'].replace(\"<|end_of_text|>\", \"\") == outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4140f47-b8a3-41ee-9e81-195b7f2ef597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Bash Command:\n",
      " rm aa.txt\n"
     ]
    }
   ],
   "source": [
    "question = '\"aa.txt\" 파일을 삭제하시오.'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"너는 bash 명령어 생성 전문가야.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 생성\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    top_k=50,\n",
    "    top_p=0.1,\n",
    "    eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "    pad_token_id=pipe.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "generated_code = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "print(\"Generated Bash Command:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c40efc32-5ab8-4a76-bab0-513d7d355f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:00<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(sample):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"][:2],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True)\n",
    "    outputs = pipe(prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "        pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    return (sample[\"messages\"][1][\"content\"], predicted_answer, sample[\"messages\"][2][\"content\"])\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 809\n",
    "\n",
    "sampled_eval_dataset = eval_dataset.shuffle(seed=42).select(range(200))\n",
    "for test_data in tqdm(sampled_eval_dataset):\n",
    "    success_rate.append(evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8042fc4-d134-4718-8a7c-f9d18dc60459",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./success_rate-data-augmented-v4.txt\", \"w\") as f:\n",
    "    for result in success_rate:\n",
    "        f.write(str(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce3609e5-b177-4cd4-ae3e-fa711e8299f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_result = [temp[1] == temp[2].replace(\"<|end_of_text|>\", \"\") for temp in success_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac51f577-71b2-444d-beab-75e6193703f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.50%\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(generated_result)/len(generated_result)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c8b5ec9-eb19-44ee-a610-3d99cb040639",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = []\n",
    "with open(\"success_rate-data-augmented-v4.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        success_rate.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03ab0950-76a0-4b51-99bb-04f5a3fcd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_evaluation = [(temp[0], temp[1], temp[2].replace(\"<|end_of_text|>\", \"\")) for temp in success_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10e52f35-aaee-46ab-bc8a-5f3d78cf3f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Task: \"~\" (홈 디렉토리가 아닌)라는 이름의 디렉토리로 이동한다.', 'cd ~/', 'cd \"~\"')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31bfc545-a1d7-4bcc-80c8-ef833a68f6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\": 0, \"explanation\": \"차이점 설명:\\n- 생성된 코드 `cd \\\"~\\\"`는 문자열 `\\\"~\\\"`로 해석되어 현재 작업 디렉토리 하위에 있는 `\\\"~\\\"`라는 이름의 디렉토리로 이동하려고 시도합니다. 이는 실제 홈 디렉토리에 대한 경로가 아닙니다.\\n- 반면, 정답 코드 `cd ~/not_home`는 실제 홈 디렉토리의 `not_home`이라는 디렉토리로 이동하는 명령입니다. 여기서 `~`는 사용자의 홈 디렉토리를 의미합니다.\\n\\n따라서 두 코드의 목적지가 다르기 때문에 의미적으로 동일하지 않습니다.\"}\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def one_compare_bash_semantics(problem_description, generated_query, ground_truth_query):\n",
    "    # ChatGPT에게 물어볼 프롬프트 작성\n",
    "    prompt = f\"\"\"다음 문제와 두 Bash 코드가 의미적으로 동일한 결과를 반환하는지 판단해주세요:\n",
    "\n",
    "    문제 설명: {problem_description}\n",
    "\n",
    "    생성된 코드:\n",
    "    {generated_query}\n",
    "\n",
    "    정답 코드:\n",
    "    {ground_truth_query}\n",
    "\n",
    "    두 코드가 문제에 대해 의미적으로 동일한 결과를 반환한다면 \"Yes\"라고 대답하고,\n",
    "    그렇지 않다면 \"No\"라고 대답한 후 차이점을 설명해주세요.\n",
    "    코드의 구조나 사용된 함수가 다르더라도 결과가 같다면 의미적으로 동일하다고 판단해주세요.\"\"\"\n",
    "\n",
    "    # ChatGPT API 호출\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # 또는 사용 가능한 최신 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that compares the semantic meaning of Bash codes in the context of a given problem.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ChatGPT의 응답 추출\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 결과 처리\n",
    "    is_correct = 1 if answer.lower().startswith(\"yes\") else 0\n",
    "    explanation = answer[3:] if is_correct == 1 else answer[2:]\n",
    "\n",
    "    # JSON 형식으로 결과 반환\n",
    "    result = {\n",
    "        \"answer\": is_correct,\n",
    "        \"explanation\": explanation.strip()\n",
    "    }\n",
    "\n",
    "    return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "# 사용 예시\n",
    "\n",
    "problem = openai_evaluation[1][0]\n",
    "truth = openai_evaluation[1][1]\n",
    "generated = openai_evaluation[1][2]\n",
    "\n",
    "result = one_compare_bash_semantics(problem, generated, truth)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e14c9042-2b89-4ffc-b1cd-b46177c3d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8dca0229-386b-40ef-b0e4-7a092d10e54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af95864396564ec79b34f9991975a603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3b2e3209974b6199a3cecd2112c96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 175 (char 195)\n",
      "[31] 원본 응답 내용: '{\\n    \"answer\": \"1\",\\n    \"explanation\": \"생성된 코드와 정답 코드 모두 \\'find\\' 명령어를 사용하여 ~/junk 디렉토리 내에서 \\'cart1\\'이라는 이름의 파일을 찾고, 해당 파일을 ~/junk/A 디렉토리로 이동시키는 작업을 수행합니다. 두 코드의 차이점은 \\'-name\\' 옵션에 사용된 인용문자가 큰따옴표(\\'\\\\\"\\')인지 작은따옴표(\\'\\\\\\'\\')인지의 차이뿐이며, Bash에서는 \\'find\\' 명령어에서 이름을 지정할 때 둘 모두 기능적으로 동일하게 동작합니다. 따라서 두 코드가 문제에 대해 의미적으로 동일한 결과를 반환합니다.\"\\n}'\n",
      "[91] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 225 (char 245)\n",
      "[91] 원본 응답 내용: '{\\n    \"answer\": \"1\",\\n    \"explanation\": \"Both Bash codes are using the \\'find\\' command to locate files or directories with the extension .txt in the current directory and its subdirectories. The only difference is in the use of double quotes (\\\\\") versus single quotes (\\\\\\'), which in this context does not affect the outcome of the command. Double quotes and single quotes can be used interchangeably here because there are no special characters or variables that require special treatment. Thus, both commands yield the same result and are semantically equivalent for the given task.\"\\n}'\n",
      "[86] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 58 (char 78)\n",
      "[86] 원본 응답 내용: '```json\\n{\\n    \"answer\": \"0\",\\n    \"explanation\": \"The generated code \\'find -newermt \\\\\"Mar 03\\\\\"\\' searches for all files that have been modified after March 3rd of the current year, disregarding the time and assumes the search starts from the current directory. The correct answer code, \\'find . -type f -newermt \\\\\"$(date \\\\\\'+%Y-%m-%d %H:%M:%S\\\\\\' -d @1494500000)\\\\\"\\', is searching for files starting from the current directory and using a specific epoch time (1494500000) which corresponds to a precise date and time in history for comparison. Since \\'Mar 03\\' could refer to any year and does not include time, these two commands will not yield the same set of files. Therefore, they are not semantically equivalent for a specific task.\"\\n}\\n```'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b733eae3b84b149af962866e489d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from pqdm.processes import pqdm\n",
    "\n",
    "client = OpenAI()\n",
    "import re\n",
    "\n",
    "def extract_json_from_markdown(text):\n",
    "    if text.strip().startswith(\"```json\"):\n",
    "        text = re.sub(r\"^```json\\s*\", \"\", text.strip())\n",
    "        text = re.sub(r\"\\s*```$\", \"\", text.strip())\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        text = text.replace('\\\\', '\\\\\\\\')  # \\ → \\\\\n",
    "        return json.loads(text)\n",
    "\n",
    "\n",
    "def compare_bash_semantics(idx):\n",
    "    save_path = f\"./results-v5/result_{idx}.json\"\n",
    "    if Path(save_path).exists():\n",
    "        print(\"이미 처리된 파일입니다.\")\n",
    "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)  \n",
    "    else:\n",
    "        item = openai_evaluation[idx]\n",
    "        problem_description, generated_query, ground_truth_query = item\n",
    "\n",
    "        prompt = f\"\"\"다음 문제와 두 Bash 코드가 의미적으로 동일한 결과를 반환하는지 판단해주세요:\n",
    "\n",
    "        문제 설명: {problem_description}\n",
    "\n",
    "        생성된 코드:\n",
    "        {generated_query}\n",
    "\n",
    "        정답 코드:\n",
    "        {ground_truth_query}\n",
    "\n",
    "        두 코드가 문제에 대해 의미적으로 동일한 결과를 반환한다면 answer에 \"1\"라고 대답하고,\n",
    "        그렇지 않다면 \"0\"라고 대답한 후 차이점을 explanation에 적으세요.\n",
    "        코드의 구조나 사용된 함수가 다르더라도 결과가 같다면 의미적으로 동일하다고 판단해주세요.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant that compares the semantic meaning of Bash codes in the context of a given problem.\n",
    "                반드시 아래 형식으로만 응답하세요:\n",
    "                {\n",
    "                    \"answer\": \"...\",\n",
    "                    \"explanation\": \"...\"\n",
    "                }\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            raw_content = response.choices[0].message.content\n",
    "            parsed = extract_json_from_markdown(raw_content)\n",
    "        \n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(parsed, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "            return parsed\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] OpenAI 응답 파싱 실패: {e}\")\n",
    "            print(f\"[{idx}] 원본 응답 내용: {raw_content!r}\")\n",
    "        \n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"answer\": \"0\",\n",
    "                    \"explanation\": f\"파싱 실패: {str(e)}\\n원본 응답: {raw_content}\"\n",
    "                }, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "            return {\n",
    "                \"answer\": \"0\",\n",
    "                \"explanation\": f\"파싱 실패: {str(e)}\\n원본 응답: {raw_content}\"\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "# generated_result에 인덱스 추가\n",
    "indexed_openai_evaluation = list(range(len((openai_evaluation))))\n",
    "\n",
    "# pqdm을 사용하여 병렬 처리\n",
    "results = pqdm(indexed_openai_evaluation, compare_bash_semantics, n_jobs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df63cb02-2216-4f9a-a812-bfe304f514d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': '0',\n",
       "  'explanation': \"The generated code 'find /proc | xargs ls' and the correct code 'find /proc -exec ls '{}' \\\\;' are not semantically the same for this task. The problem is that the generated code may not correctly handle file names with spaces or special characters because xargs by default splits on whitespace, whereas the correct code handles each entry individually with '-exec ls'. In addition, the generated code might also fail if there are too many files due to argument length limits for xargs, which isn't an issue with the '-exec' approach as it's called for each file separately. Therefore, they can produce different results in such cases.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': 'The generated code \\'cd ~/\\' attempts to change the directory to the home directory, since \\'~/\\' refers to the user\\'s home directory. The correct code \\'cd \"~\"\\' tries to change the directory to a directory literally named \\'~\\'. The two commands are semantically different because they point to different directories. One points to the home directory, while the other points to a directory named \\'~\\'.'},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code is looking for directories in /raid with a name ending in '.local_sd_customize', whereas the correct code is looking for directories with the exact name '.local_sd_customize'. The use of the wildcard '*' means the generated code can match directories like 'example.local_sd_customize' which would not be matched by the correct code that looks for '.local_sd_customize' exactly. Therefore, they do not return the same results.\"},\n",
       " {'answer': '1',\n",
       "  'explanation': 'The \\'find\\' command defaults to executing the \\'-print\\' action if no action is specified. Therefore, the generated code \\'find . -type f \\\\( -name \"*.conf\" -or -name \"*.txt\" \\\\)\\' and the correct code \\'find . -type f \\\\( -name \"*.conf\" -or -name \"*.txt\" \\\\) -print\\' are semantically equivalent. Both commands will search for files ending in .conf or .txt in the current directory tree and print their paths.'},\n",
       " {'answer': '0',\n",
       "  'explanation': 'The generated code has a syntax error: `find . -name \"*shp*\" -exec mv {}../shp_all/ \\\\;` has a missing space between `{}` and `../shp_all/`. This will cause the command to incorrectly interpret the path, likely causing errors or incorrect behavior. The correct syntax should include a space between the source and destination paths in the `mv` command, as shown in the correct code: `find . -name \"*shp*\" -exec mv {} ../shp_all/ \\\\;`. Thus, the behavior of the generated code is not semantically equivalent to the correct code.'},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The two code snippets do not achieve the same goal. The generated code snippet uses 'find' to get files matching the pattern and then uses 'tail' to get only the last file found. This file is then archived into 'file.tar'. In contrast, the 'correct' code uses 'find' with '-exec' to immediately archive every file that matches the pattern into 'file.tar', overwriting previous files. This means it does not specifically handle just the last found file — it archives each file it finds. Additionally, the 'correct' code does not create a compressed tarball and overwrites 'file.tar' each time a matching file is found.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code 'find . -type f -perm -o=r' searches for regular files that are world-readable (i.e., can be read by others), which matches the task description. However, the provided correct code 'find . -perm -g=r -type f -exec ls -l {} \\\\;' searches for files that are group-readable (i.e., can be read by the group) and then lists their details with 'ls -l'. The two codes are not equivalent because one checks for group readability while the other checks for world readability.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code uses -iname 'test*', which matches files that start with 'test', ignoring case. The correct code uses -iname '*test*', which matches files that contain 'test' anywhere in their name, also ignoring case. Therefore, they will not return the same files if a file name contains 'test' but does not start with it.\"},\n",
       " {'answer': '1',\n",
       "  'explanation': \"두 코드 모두 현재 폴더에서 지난 60분 안에 수정된 모든 정규 파일의 상세 목록을 보여주는 명령입니다. 생성된 코드는 'find' 명령어를 사용하여 시간 조건에 맞는 파일들을 찾고, '-exec ls -l {} \\\\;'를 사용하여 각 파일에 대해 'ls -l' 명령을 실행하여 상세 정보를 출력합니다. 정답 코드는 'find' 명령어의 '-ls' 옵션을 사용하여 시간 조건에 맞는 파일들의 상세 정보를 직접 출력합니다. 두 방법 모두 같은 결과를 얻습니다.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': 'The generated code and the provided solution code have different behaviors. The generated code uses \\'find\\' with \\'-name \"*.txt\"\\' to find files that end with \\'.txt\\', while the answer code uses \\'.txt\\', which will not match any files (it misses the wildcard \\'*\\'). The generated code uses \\'rm -rf\\' which attempts to remove files and directories recursively, although \\'-f\\' is redundant for files. The answer code uses \\'rm\\' without options, which will only remove files. Thus, the generated code will correctly find and delete text files, while the answer code will not find any files due to missing \\'*\\'.'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84cba7e9-fbea-42f3-abf5-7b0f03e09100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee99169-a11e-4314-9a40-f7492d141f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.00%\n"
     ]
    }
   ],
   "source": [
    "json_result = []\n",
    "for result in results:\n",
    "    json_result.append(result)\n",
    "\n",
    "df = pd.DataFrame(json_result)\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].map(lambda x : int(x))\n",
    "\n",
    "after_accuracy = df[\"answer\"].sum() / len(df[\"answer\"])\n",
    "print(f\"Accuracy: {after_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a78f4-97e3-44f9-854a-6680b88c6247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
