{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2ffb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers bitsandbytes datasets sentencepiece accelerate peft flash-attn wandb openai pqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a4e378-3792-45c2-aef7-3b105610475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (4.13.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting trl==0.9.6\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (2.4.1+cu124)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (4.51.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (1.26.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (1.6.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (3.5.1)\n",
      "Collecting tyro>=0.5.11 (from trl==0.9.6)\n",
      "  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (4.67.1)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->trl==0.9.6) (6.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2024.8.30)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.16.0)\n",
      "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Downloading tyro-0.9.19-py3-none-any.whl (124 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: typeguard, shtab, mdurl, docstring-parser, markdown-it-py, rich, tyro, trl\n",
      "Successfully installed docstring-parser-0.16 markdown-it-py-3.0.0 mdurl-0.1.2 rich-14.0.0 shtab-1.7.2 trl-0.9.6 typeguard-4.4.2 tyro-0.9.19\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U typing_extensions\n",
    "!pip install trl==0.9.6\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc022a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import trl\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "\n",
    "import wandb\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          pipeline)\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18bc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version       : 2.4.1+cu124\n",
      "Transformers version  : 4.51.3\n",
      "TRL version           : 0.9.6\n",
      "CUDA available        : True\n",
      "CUDA version      : 12.4\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version       : {torch.__version__}\")\n",
    "print(f\"Transformers version  : {transformers.__version__}\")\n",
    "print(f\"TRL version           : {trl.__version__}\")\n",
    "print(f\"CUDA available        : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version      : {torch.version.cuda}\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "login(\n",
    "  token=HUGGINGFACE_TOKEN,\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e52a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 8089\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"rlawltjd/korean-nl2bash\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33fb13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecd4bb06e304fd882262528ba27a130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b2819920f5434f8623355b015d2316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful programmer assistant that excels at changing Korean text to Bash.', 'role': 'system'}, {'content': 'Task: 모든 *.java 파일에서 StringBuffer 찾기', 'role': 'user'}, {'content': 'find . -type f -name \"*.java\" -exec grep -l StringBuffer {} \\\\;<|end_of_text|>', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allganize/Llama-3-Alpha-Ko-8B-Instruct\")\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "def get_chat_format(element):\n",
    "    system_prompt = \"You are a helpful programmer assistant that excels at changing Korean text to Bash.\"\n",
    "    user_prompt = \"Task: {instruction}\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format_map(element)},\n",
    "            {\"role\": \"assistant\", \"content\": element[\"output\"] + tokenizer.eos_token},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# 데이터를 일괄적으로 대화 형식으로 변경하는 코드\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    get_chat_format,\n",
    "    remove_columns=[\"instruction\", \"output\"],\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.05)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"test\": split_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(dataset[\"train\"][345][\"messages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8012c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 7684\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 405\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36515f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful programmer assistant that excels at changing Korean text to Bash.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Task: \"txt\"로 끝나지 않는 현재 디렉토리 트리의 모든 일반 파일을 제거합니다.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"find . -type f -not -name '*txt' -print0 | xargs -0 rm --<|end_of_text|>\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475cdbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b45dc55f4040dea8f6ecfea3f643d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb30fb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c237a016c904e00a54becca585ff7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantization config 세팅 -> 모델이 사용하는 vram을 최소화\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "model_id = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,                                     \n",
    "    device_map=\"auto\",                            \n",
    "    attn_implementation=\"flash_attention_2\",        \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config                  \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)   \n",
    "tokenizer.padding_side = 'right'                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524576d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df9525e62e34bd7bfc370fad602821d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,                            \n",
    "        lora_dropout=0.05,                         \n",
    "        r=256,                                     # Lora의 저차원 공간의 랭크를 지정. 랭크가 높을수록 모델의 표현력과과 계산 비용도 증가.\n",
    "        bias=\"none\",                               # Lora 적용 시 바이어스를 사용할지 여부를 설정. \n",
    "        target_modules=[\"q_proj\", \"o_proj\",        # Lora를 적용할 모델의 모듈 리스트\n",
    "                        \"k_proj\", \"v_proj\",\n",
    "                        \"up_proj\", \"down_proj\",\n",
    "                        \"gate_proj\",\n",
    "                        ],\n",
    "        task_type=\"CAUSAL_LM\",                    \n",
    ")\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"code-llama-7b-text-to-bash-v3\", \n",
    "    num_train_epochs=5,                   \n",
    "    # max_steps=100,                          \n",
    "    per_device_train_batch_size=1,         \n",
    "    gradient_accumulation_steps=2,          \n",
    "    gradient_checkpointing=True,            \n",
    "    optim=\"adamw_torch_fused\",              # 메모리 효율화할 수 있는 fused AdamW 옵티마이저 사용.\n",
    "    logging_steps=10,                       \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-4,                     # 학습률 2e-4로 설정 (QLoRA 논문 기반).\n",
    "    bf16=True,                              \n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,                      \n",
    "    warmup_ratio=0.03,                      # 워밍업 비율 0.03으로 설정 (QLoRA 논문 기반).\n",
    "    lr_scheduler_type=\"cosine\",           \n",
    "    push_to_hub=True,                       \n",
    "    report_to=\"wandb\",                      \n",
    ")\n",
    "\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,                     \n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  \n",
    "        \"append_concat_token\": False, \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef811a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoka27\u001b[0m (\u001b[33msoka27-hufs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/kotext-to-bash/wandb/run-20250504_114923-y2gv2krw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/soka27-hufs/huggingface/runs/y2gv2krw' target=\"_blank\">code-llama-7b-text-to-bash-v3</a></strong> to <a href='https://wandb.ai/soka27-hufs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/soka27-hufs/huggingface' target=\"_blank\">https://wandb.ai/soka27-hufs/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/soka27-hufs/huggingface/runs/y2gv2krw' target=\"_blank\">https://wandb.ai/soka27-hufs/huggingface/runs/y2gv2krw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 19:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.712100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.4238887878025279, metrics={'train_runtime': 1167.0027, 'train_samples_per_second': 0.583, 'train_steps_per_second': 0.291, 'total_flos': 1.3663491814588416e+17, 'train_loss': 0.4238887878025279, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde9a402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fdafc3cbe34b15be86210b5ab2bf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e15928bdfec43678ca016fa370c7773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e7eaa0c12b4d578a79b2e5b9991eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20629a7b61374fe68626202381fe3297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rlawltjd/code-llama3-7B-text-to-bash-v3/commit/b05b3e5da175eb06daf42ac5436387b7e55d91e8', commit_message='Upload tokenizer', commit_description='', oid='b05b3e5da175eb06daf42ac5436387b7e55d91e8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rlawltjd/code-llama3-7B-text-to-bash-v3', endpoint='https://huggingface.co', repo_type='model', repo_id='rlawltjd/code-llama3-7B-text-to-bash-v3'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"rlawltjd/code-llama3-7B-text-to-bash-v3\")\n",
    "tokenizer.push_to_hub(\"rlawltjd/code-llama3-7B-text-to-bash-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd89951-b36d-42d9-a2bb-b3ae8faab5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d31efa-b17b-423f-ab83-973da3be0200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d26d96d0b14b3a9c23fa29dfccc46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"./code-llama-7b-text-to-bash-v3\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5570c1a6-5e6c-4193-8861-2d8e845050b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb321a4561914c09a897503f39b8ddc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Task: 'in.txt' 파일에서 정규 표현식 \"+\\S\\+\"를 검색하고, 쉼표(',')로 새 줄을 대체하여 일치하는 항목을 출력합니다.\n",
      "Original Answer:\n",
      "grep -o \"+\\S\\+\" in.txt | tr '\\n' ','<|end_of_text|>\n",
      "Generated Answer:\n",
      "cat in.txt | grep -o \",+\\S\\+\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    eval_dataset[rand_idx][\"messages\"][:2], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "outputs = pipe(prompt, \n",
    "               max_new_tokens=256, \n",
    "               do_sample=False, \n",
    "               temperature=0.1, \n",
    "               top_k=50, \n",
    "               top_p=0.1, \n",
    "               eos_token_id=pipe.tokenizer.eos_token_id, \n",
    "               pad_token_id=pipe.tokenizer.pad_token_id\n",
    "               )\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\".replace(\"<|im_end|>\", \"\"))\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "eval_dataset[rand_idx]['messages'][2]['content'].replace(\"<|end_of_text|>\", \"\") == outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4140f47-b8a3-41ee-9e81-195b7f2ef597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Bash Command:\n",
      " rm aa.txt\n"
     ]
    }
   ],
   "source": [
    "question = '\"aa.txt\"를 삭제하시오.'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"너는 bash 명령어 생성 전문가야.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 생성\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    top_k=50,\n",
    "    top_p=0.1,\n",
    "    eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "    pad_token_id=pipe.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "generated_code = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "print(\"Generated Bash Command:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c40efc32-5ab8-4a76-bab0-513d7d355f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/405 [00:11<09:02,  1.37s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 405/405 [06:04<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(sample):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"][:2],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True)\n",
    "    outputs = pipe(prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "        pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    return (sample[\"messages\"][1][\"content\"], predicted_answer, sample[\"messages\"][2][\"content\"])\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 405\n",
    "\n",
    "sampled_eval_dataset = eval_dataset.shuffle(seed=42).select(range(405))\n",
    "for test_data in tqdm(sampled_eval_dataset):\n",
    "    success_rate.append(evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8042fc4-d134-4718-8a7c-f9d18dc60459",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./success_rate-v3.txt\", \"w\") as f:\n",
    "    for result in success_rate:\n",
    "        f.write(str(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3609e5-b177-4cd4-ae3e-fa711e8299f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_result = [temp[1] == temp[2].replace(\"<|end_of_text|>\", \"\") for temp in success_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac51f577-71b2-444d-beab-75e6193703f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 12.59%\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(generated_result)/len(generated_result)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c8b5ec9-eb19-44ee-a610-3d99cb040639",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = []\n",
    "with open(\"success_rate-v3.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        success_rate.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ab0950-76a0-4b51-99bb-04f5a3fcd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_evaluation = [(temp[0], temp[1], temp[2].replace(\"<|end_of_text|>\", \"\")) for temp in success_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10e52f35-aaee-46ab-bc8a-5f3d78cf3f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Task: '/var/www' 디렉토리 트리 아래의 모든 디렉토리를 찾되, '/var/www/web-release-data'와 '/var/www/web-development-data' 디렉토리와 그들의 하위 디렉토리는 제외해라.\",\n",
       " 'find /var/www -type d \\\\(! -wholename \"/var/www/web-release-data/*\" -a! -wholename \"/var/www/web-development-data/*\" \\\\)',\n",
       " 'find /var/www -type d \\\\( ! -wholename \"/var/www/web-release-data/*\"  ! -wholename \"/var/www/web-development-data/*\" \\\\)')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31bfc545-a1d7-4bcc-80c8-ef833a68f6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\": 0, \"explanation\": \"차이점 설명:\\n두 코드의 `find` 명령어 사용에서 조건의 결합 방식에 차이가 있습니다.\\n\\n1. **생성된 코드**:\\n   ```bash\\n   find /var/www -type d \\\\( ! -wholename \\\"/var/www/web-release-data/*\\\"  ! -wholename \\\"/var/www/web-development-data/*\\\" \\\\)\\n   ```\\n   이 코드는 중괄호 내에서 두 개의 조건을 나열하고 있습니다. 각 조건은 다른 디렉토리를 제외시키기 위해 `! -wholename` 절을 사용합니다. 그러나 이 경우 두 조건이 `AND`로 결합되지 않고 독립적으로 평가됩니다. 여기서 두 조건이 모두 true이어야 최종 조건이 true가 되지 않아, 원하는 결과가 아닐 수 있습니다.\\n\\n2. **정답 코드**:\\n   ```bash\\n   find /var/www -type d \\\\(! -wholename \\\"/var/www/web-release-data/*\\\" -a ! -wholename \\\"/var/www/web-development-data/*\\\" \\\\)\\n   ```\\n   이 코드는 `-a` (AND) 연산자를 사용하여 두 조건을 명시적으로 결합하고 있습니다. 즉, 이 코드는 두 디렉토리에 대해 모두 제외되어야 최종 조건이 true가 되도록 설정되어 있습니다.\\n\\n결과적으로, 생성된 코드에서는 한 조건이 참일 경우 다른 조건이 어떤 값을 가질지에 상관없이 결과가 영향을 받을 수 있지만, 정답 코드에서는 두 조건이 모두 만족해야 최종적으로 포함되지 않게 됩니다. 따라서, 두 코드는 의미적으로 동일한 결과를 반환하지 않습니다.\"}\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def one_compare_bash_semantics(problem_description, generated_query, ground_truth_query):\n",
    "    # ChatGPT에게 물어볼 프롬프트 작성\n",
    "    prompt = f\"\"\"다음 문제와 두 Bash 코드가 의미적으로 동일한 결과를 반환하는지 판단해주세요:\n",
    "\n",
    "    문제 설명: {problem_description}\n",
    "\n",
    "    생성된 코드:\n",
    "    {generated_query}\n",
    "\n",
    "    정답 코드:\n",
    "    {ground_truth_query}\n",
    "\n",
    "    두 코드가 문제에 대해 의미적으로 동일한 결과를 반환한다면 \"Yes\"라고 대답하고,\n",
    "    그렇지 않다면 \"No\"라고 대답한 후 차이점을 설명해주세요.\n",
    "    코드의 구조나 사용된 함수가 다르더라도 결과가 같다면 의미적으로 동일하다고 판단해주세요.\"\"\"\n",
    "\n",
    "    # ChatGPT API 호출\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # 또는 사용 가능한 최신 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that compares the semantic meaning of Bash codes in the context of a given problem.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ChatGPT의 응답 추출\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 결과 처리\n",
    "    is_correct = 1 if answer.lower().startswith(\"yes\") else 0\n",
    "    explanation = answer[3:] if is_correct == 1 else answer[2:]\n",
    "\n",
    "    # JSON 형식으로 결과 반환\n",
    "    result = {\n",
    "        \"answer\": is_correct,\n",
    "        \"explanation\": explanation.strip()\n",
    "    }\n",
    "\n",
    "    return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "# 사용 예시\n",
    "\n",
    "problem = openai_evaluation[1][0]\n",
    "truth = openai_evaluation[1][1]\n",
    "generated = openai_evaluation[1][2]\n",
    "\n",
    "result = one_compare_bash_semantics(problem, generated, truth)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14c9042-2b89-4ffc-b1cd-b46177c3d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dca0229-386b-40ef-b0e4-7a092d10e54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fecd415eaaf4082a6ff3b18a60a1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bd1ac7205b461c8a95b7fabfda0baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 135 (char 155)\n",
      "[112] 원본 응답 내용: '{\\n    \"answer\": \"0\",\\n    \"explanation\": \"The generated code and the correct code are not semantically equivalent. The generated code uses \\'find . -name \\\\\"*.jpg\\\\\" -type f -exec du -h \\'{}\\' \\\\+ | tail -n 1\\', which finds all jpg files and gets individual sizes, outputting the human-readable size of the last file. The correct code \\'find . -type f -iname \\'*.jpg\\' -print0 | du -c --files0-from=-\\' computes the total size of all jpg files together. The generated code results in a single file size, whereas the correct code results in the total size, which is the answer to the problem.\"\\n}'\n",
      "[220] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 52 (char 72)\n",
      "[220] 원본 응답 내용: '```json\\n{\\n    \"answer\": \"0\",\\n    \"explanation\": \"The first code \\'find. -name \\\\\"file.ext\\\\\" -execdir pwd \\\\\\';\\\\\\'\\' has a syntax error because there is no space between \\'find\\' and \\'.\\', making it an invalid command. This would not run correctly and result in an error if executed in a shell. On the other hand, the second code \\'find . -name \\\\\"file.ext\\\\\" -execdir pwd \\\\\\';\\\\\\'\\' is correct and will find all files named \\'file.ext\\' and print their directory paths. Therefore, they are not semantically equivalent.\"\\n}\\n```'\n",
      "[334] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 634 (char 654)\n",
      "[334] 원본 응답 내용: '```json\\n{\\n    \"answer\": \"0\",\\n    \"explanation\": \"The generated code uses `find . -type d -name \\'/svn\\' -o -name \\'*.txt\\'` which incorrectly interprets the logical operation. It looks for directories named \\'/svn\\' and files named \\'*.txt\\', but due to operator precedence without proper grouping, it does not exclude files inside \\'/svn\\'. Also, the generated code uses `-type d` which will look for directories instead of excluding directories during the search. The command given doesn\\'t ensure files inside \\'/svn\\' are excluded, and it doesn\\'t navigate subdirectories correctly. This means text files inside \\'/svn\\' might not be excluded properly. The use of `grep -n \"pattern\"` with `xargs` will fail to handle filenames with spaces correctly as well if `find` locates them. The correct code, on the other hand, uses `\\\\! -wholename \\'*/.svn/*\\'` to exclude paths containing \\'/svn\\' properly, ensuring no text files from \\'/svn\\' are processed. Additionally, the correct code directly uses `grep` with `-exec` and avoids xargs, further ensuring proper handling of filenames.\"\\n}\\n```'\n",
      "[387] OpenAI 응답 파싱 실패: Expecting ',' delimiter: line 3 column 199 (char 219)\n",
      "[387] 원본 응답 내용: '{\\n    \"answer\": \"0\",\\n    \"explanation\": \"The generated code and the correct code provide different filtering criteria for file names. The generated code searches for files with names starting with \\'.\\' and ending with \\'\"\\' (literally as stated perhaps mistakenly with quotes), whereas the correct code excludes files with names starting with \\'.\\'. Additionally, the generated code uses \\'-name \\\\\"*\\\\\"\\', which is incorrect, as it should match a pattern that\\'s specific from starting with \\'.\\' and ending with \\'\\\\\"\\'. The correct code inversely checks using \\'! -name\\', effectively looking for files not following a specific pattern but is indeed still incorrect given the problem statement reads that filename should end with double quotes. The problem dictates starting with \\'.\\' and ending with double quotes. However, neither codes correctly captures this explicitly in isolation why).\" \\n}'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5d57d4fe844f0e94b12abc3f1c3095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from pqdm.processes import pqdm\n",
    "\n",
    "client = OpenAI()\n",
    "import re\n",
    "\n",
    "def extract_json_from_markdown(text):\n",
    "    if text.strip().startswith(\"```json\"):\n",
    "        text = re.sub(r\"^```json\\s*\", \"\", text.strip())\n",
    "        text = re.sub(r\"\\s*```$\", \"\", text.strip())\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        text = text.replace('\\\\', '\\\\\\\\')  # \\ → \\\\\n",
    "        return json.loads(text)\n",
    "\n",
    "\n",
    "def compare_bash_semantics(idx):\n",
    "    save_path = f\"./results-v3/result_{idx}.json\"\n",
    "    if Path(save_path).exists():\n",
    "        print(\"이미 처리된 파일입니다.\")\n",
    "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)  \n",
    "    else:\n",
    "        item = openai_evaluation[idx]\n",
    "        problem_description, generated_query, ground_truth_query = item\n",
    "\n",
    "        prompt = f\"\"\"다음 문제와 두 Bash 코드가 의미적으로 동일한 결과를 반환하는지 판단해주세요:\n",
    "\n",
    "        문제 설명: {problem_description}\n",
    "\n",
    "        생성된 코드:\n",
    "        {generated_query}\n",
    "\n",
    "        정답 코드:\n",
    "        {ground_truth_query}\n",
    "\n",
    "        두 코드가 문제에 대해 의미적으로 동일한 결과를 반환한다면 answer에 \"1\"라고 대답하고,\n",
    "        그렇지 않다면 \"0\"라고 대답한 후 차이점을 explanation에 적으세요.\n",
    "        코드의 구조나 사용된 함수가 다르더라도 결과가 같다면 의미적으로 동일하다고 판단해주세요.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant that compares the semantic meaning of Bash codes in the context of a given problem.\n",
    "                반드시 아래 형식으로만 응답하세요:\n",
    "                {\n",
    "                    \"answer\": \"...\",\n",
    "                    \"explanation\": \"...\"\n",
    "                }\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            raw_content = response.choices[0].message.content\n",
    "            parsed = extract_json_from_markdown(raw_content)\n",
    "        \n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(parsed, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "            return parsed\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] OpenAI 응답 파싱 실패: {e}\")\n",
    "            print(f\"[{idx}] 원본 응답 내용: {raw_content!r}\")\n",
    "        \n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"answer\": \"0\",\n",
    "                    \"explanation\": f\"파싱 실패: {str(e)}\\n원본 응답: {raw_content}\"\n",
    "                }, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "            return {\n",
    "                \"answer\": \"0\",\n",
    "                \"explanation\": f\"파싱 실패: {str(e)}\\n원본 응답: {raw_content}\"\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "# generated_result에 인덱스 추가\n",
    "indexed_openai_evaluation = list(range(len((openai_evaluation))))\n",
    "\n",
    "# pqdm을 사용하여 병렬 처리\n",
    "results = pqdm(indexed_openai_evaluation, compare_bash_semantics, n_jobs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df63cb02-2216-4f9a-a812-bfe304f514d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': '0',\n",
       "  'explanation': '\"생성된 코드\"는 \\'-iname\\' 옵션을 사용하여 대소문자를 구분하지 않고 \"Tecmint\"라는 이름의 디렉토리를 찾습니다. 이는 \\'tecmint\\', \\'TECMINT\\' 등과 같은 이름도 모두 찾게 됩니다. 반면, \"정답 코드\"는 \\'-name\\' 옵션을 사용하여 대소문자를 구분하여 정확히 \\'Tecmint\\'라는 이름의 디렉토리만을 찾습니다. 따라서 두 코드는 문제에 대해 의미적으로 동일한 결과를 반환하지 않습니다.'},\n",
       " {'answer': '1',\n",
       "  'explanation': 'Both codes use the `find` command to search for directories under /var/www while excluding certain subdirectories. The only difference between the two is the presence of the `-a` operator in the generated code. In the context of `find`, conditions within parentheses are implicitly ANDed together, making the `-a` operator redundant. Thus, both codes are functionally equivalent in achieving the task of excluding directories with names starting with /var/www/web-release-data and /var/www/web-development-data.'},\n",
       " {'answer': '1',\n",
       "  'explanation': \"Both codes find all regular files under the specified directory and change their permissions to 640. The difference between ';' and '+' in the '-exec' option is that ';' executes 'chmod' for each file individually, whereas '+' groups multiple files into a single 'chmod' command. Despite the different execution efficiency, both achieve the same final result of setting the file permissions to 640.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code uses `tail -n +1 filename | grep -v ^$` which outputs all lines from the file that are not empty, starting from the first line. It does not specifically target the last non-empty line. On the other hand, the correct solution `tac $FILE | grep -m 1 '.'` reverses the file's content and finds the first non-empty line from the reversed output, effectively returning the last non-empty line of the original file. The generated code does not fulfill the task of returning the last non-empty line specifically, unlike the correct solution.\"},\n",
       " {'answer': '1',\n",
       "  'explanation': \"두 코드 모두 의미적으로 동일한 결과를 반환합니다. 생성된 코드와 정답 코드는 모두 입력 파일 '$1'을 최대 '$2' 줄 (혹은 기본값 10000 줄)로 분할하며, 결과 파일의 접두사는 '${tdir}/x'이고 6자리 숫자 접미사를 사용합니다. 인자의 순서만 다를 뿐 같은 매개변수와 옵션을 사용하고 있습니다.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code searches for files named 'DS_Store' starting from the root directory ('/'), whereas the correct code searches for files named '.DS_Store' starting from the current directory ('.'). Moreover, Mac OS X uses '.DS_Store' files, not 'DS_Store'. Therefore, the generated code may not find and remove the intended '.DS_Store' files. Additionally, the starting point for the search ('/' vs '.') is different, affecting the scope of the search.\"},\n",
       " {'answer': '1',\n",
       "  'explanation': \"Both commands find all regular files within the 'FOLDER1' directory. The use of '-print0' in the second command makes it suitable for processing filenames with special characters by other programs, but it does not affect the task of simply displaying regular files. Therefore, both commands are semantically equivalent for the given task.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code and the correct code use rsync differently. The generated code uses '-avz --progress', which means it will archive, be verbose, compress during transfer, and show progress, but it does not use the '--delete' option. The correct code uses '-az --delete', which archives, compresses, and deletes files from the destination that are not present in the source. The '--delete' option in the correct code ensures that unnecessary files at the destination are removed, which is a part of the task description. The generated code will not delete unnecessary files from the destination, making them semantically different for the given task.\"},\n",
       " {'answer': '0',\n",
       "  'explanation': \"The generated code and the correct code both aim to find text files while excluding certain directories. However, the directories being excluded differ between the two codes. The generated code excludes './ignored_directory/*' and './another_ignored_directory/*', while the correct code excludes './Movies/*', './Downloads/*', and './Music/*'. Therefore, they are not semantically equivalent since they exclude different paths and would yield different results depending on the directory contents.\"},\n",
       " {'answer': '1',\n",
       "  'explanation': \"Both the generated code and the correct code use the 'find' command to search within the '/home' directory for files belonging to the group 'test'. Therefore, they are semantically equivalent and will yield the same result.\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84cba7e9-fbea-42f3-abf5-7b0f03e09100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ee99169-a11e-4314-9a40-f7492d141f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.90%\n"
     ]
    }
   ],
   "source": [
    "json_result = []\n",
    "for result in results:\n",
    "    json_result.append(result)\n",
    "\n",
    "df = pd.DataFrame(json_result)\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].map(lambda x : int(x))\n",
    "\n",
    "after_accuracy = df[\"answer\"].sum() / len(df[\"answer\"])\n",
    "print(f\"Accuracy: {after_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a78f4-97e3-44f9-854a-6680b88c6247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
